{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74476757-55a4-487a-a9a9-fda2addcf136",
   "metadata": {},
   "source": [
    "# HF Transformers 核心模块学习：Pipelines\n",
    "\n",
    "**Pipelines**（管道）是使用模型进行推理的一种简单易上手的方式。\n",
    "\n",
    "这些管道是抽象了 Transformers 库中大部分复杂代码的对象，提供了一个专门用于多种任务的简单API，包括**命名实体识别、掩码语言建模、情感分析、特征提取和问答**等。\n",
    "\n",
    "\n",
    "| Modality                    | Task                         | Description                                                | Pipeline API                                  |\n",
    "| --------------------------- | ---------------------------- | ---------------------------------------------------------- | --------------------------------------------- |\n",
    "| Audio                       | Audio classification         | 为音频文件分配一个标签                                     | pipeline(task=“audio-classification”)         |\n",
    "|                             | Automatic speech recognition | 将音频文件中的语音提取为文本                               | pipeline(task=“automatic-speech-recognition”) |\n",
    "| Computer vision             | Image classification         | 为图像分配一个标签                                         | pipeline(task=“image-classification”)         |\n",
    "|                             | Object detection             | 预测图像中目标对象的边界框和类别                           | pipeline(task=“object-detection”)             |\n",
    "|                             | Image segmentation           | 为图像中每个独立的像素分配标签（支持语义、全景和实例分割） | pipeline(task=“image-segmentation”)           |\n",
    "| Natural language processing | Text classification          | 为给定的文本序列分配一个标签                               | pipeline(task=“sentiment-analysis”)           |\n",
    "|                             | Token classification         | 为序列里的每个 token 分配一个标签（人, 组织, 地址等等）    | pipeline(task=“ner”)                          |\n",
    "|                             | Question answering           | 通过给定的上下文和问题, 在文本中提取答案                   | pipeline(task=“question-answering”)           |\n",
    "|                             | Summarization                | 为文本序列或文档生成总结                                   | pipeline(task=“summarization”)                |\n",
    "|                             | Translation                  | 将文本从一种语言翻译为另一种语言                           | pipeline(task=“translation”)                  |\n",
    "| Multimodal                  | Document question answering  | 根据给定的文档和问题回答一个关于该文档的问题。             | pipeline(task=“document-question-answering”)  |\n",
    "|                             | Visual Question Answering    | 给定一个图像和一个问题，正确地回答有关图像的问题           | pipeline(task=“vqa”)                          |\n",
    "\n",
    "\n",
    "Pipelines 已支持的完整任务列表：https://huggingface.co/docs/transformers/task_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e908e8a-f39a-4d73-897f-07a738e6fd3a",
   "metadata": {},
   "source": [
    "## Pipeline API\n",
    "\n",
    "**Pipeline API** 是对所有其他可用管道的包装。它可以像任何其他管道一样实例化，并且降低AI推理的学习和使用成本。-\n",
    "\n",
    "- NLP(自然语言处理)\n",
    "    - Text Classification(文本分类任务)：将一个文本序列（可以是句子级别、段落或者整篇文章）标记为预定义的类别集合之一。\n",
    "        - 情感分析：根据某种极性（如积极或消极）对文本进行标记，以在政治、金融和市场等领域支持决策制定。\n",
    "        - 内容分类：根据某个主题对文本进行标记，以帮助组织和过滤新闻和社交媒体信息流中的信息（天气、体育、金融等）。\n",
    "    - Token Classification(Token分类任务)：将每个token分配一个来自预定义类别集的标签。\n",
    "        - 命名实体识别（NER）：根据实体类别（如组织、人员、位置或日期）对token进行标记。NER在生物医学设置中特别受欢迎，可以标记基因、蛋白质和药物名称。\n",
    "        - 词性标注（POS）：根据其词性（如名词、动词或形容词）对标记进行标记。POS对于帮助翻译系统了解两个相同的单词如何在语法上不同很有用（作为名词的银行与作为动词的银行）。\n",
    "    - Question Answering(问答任务)：另一个token-level的任务，返回一个问题的答案，有时带有上下文（开放领域），有时不带上下文（封闭领域）。每当我们向虚拟助手提出问题时，例如询问一家餐厅是否营业，就会发生这种情况。它还可以提供客户或技术支持，并帮助搜索引擎检索您要求的相关信息。\n",
    "        - 提取式：给定一个问题和一些上下文，模型必须从上下文中提取出一段文字作为答案\n",
    "        - 生成式：给定一个问题和一些上下文，答案是根据上下文生成的\n",
    "    - Summarization(文本摘要）：从较长的文本中创建一个较短的版本，同时尽可能保留原始文档的大部分含义。摘要是一个序列到序列的任务；它输出比输入更短的文本序列。有许多长篇文档可以进行摘要，以帮助读者快速了解主要要点。法案、法律和财务文件、专利和科学论文等文档可以摘要，以节省读者的时间并作为阅读辅助工具。\n",
    "        - 提取式：从原始文本中识别和提取最重要的句子\n",
    "        - 生成式：从原始文本中生成目标摘要（可能包括输入文件中没有的新单词）\n",
    "- Audio 音频处理任务\n",
    "    - Audio classification(音频分类)：是一项将音频数据从预定义的类别集合中进行标记的任务。\n",
    "    - Automatic speech recognition（自动语音识别）：将语音转录为文本。\n",
    "- Computer Vision 计算机视觉\n",
    "    - Image Classificaiton(图像分类)：将整个图像从预定义的类别集合中进行标记。 \n",
    "    - Object Detection(目标检测):与图像分类不同，目标检测在图像中识别多个对象以及这些对象在图像中的位置（由边界框定义）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adab2d8-5b56-40da-a7d8-df1e53825d15",
   "metadata": {},
   "source": [
    "## 对比不同模型在相同任务上的性能表现\n",
    "\n",
    "针对NLP(自然语言处理)的中文处理进行任务对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d204fb9-b4c8-4150-a2ef-e545ed4a9fcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Text Classification(文本分类任务)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c24f790c-e3cf-4bc5-aabc-5381a79c03fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'negative', 'score': 0.8286268711090088}]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 中文和英文情感分类对比\n",
    "from transformers import pipeline\n",
    "sentiment_task = pipeline(\n",
    "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", \n",
    "    top_k=1\n",
    ")\n",
    "sentiment_task(\"今儿北京可真冷啊!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e42be47a-26c2-4461-a9bf-03787123ade3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'negative', 'score': 0.7933650612831116}]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_task(\"Today Beijing is really cold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a7daaa-eef9-4bf1-b4e4-81269555450f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'positive', 'score': 0.953176736831665}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_task(\"你学东西真的好快，理论课一讲就明白了!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03bbf441-3368-4f37-84ae-10950a63c89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'positive', 'score': 0.7639099359512329}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_task(\"You learn things really quickly. You understand the theory class as soon as it is taught.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff79f94-6d15-47bc-9a0b-a594f5014ae9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Token Classification(Token分类任务)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57f059b4-f495-4fc1-a2a5-077a287fff74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-LOC',\n",
       "  'score': 0.9879116,\n",
       "  'index': 2,\n",
       "  'word': '中国的',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99999654,\n",
       "  'index': 5,\n",
       "  'word': '北京',\n",
       "  'start': 6,\n",
       "  'end': 8},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9999956,\n",
       "  'index': 8,\n",
       "  'word': '北京',\n",
       "  'start': 10,\n",
       "  'end': 12},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.999691,\n",
       "  'index': 11,\n",
       "  'word': '天',\n",
       "  'start': 15,\n",
       "  'end': 16},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99995625,\n",
       "  'index': 12,\n",
       "  'word': '安',\n",
       "  'start': 16,\n",
       "  'end': 17},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9998363,\n",
       "  'index': 13,\n",
       "  'word': '门',\n",
       "  'start': 17,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9885372,\n",
       "  'index': 22,\n",
       "  'word': '百度',\n",
       "  'start': 30,\n",
       "  'end': 32}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 中文Token分类\n",
    "from transformers import AutoTokenizer,AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"中国的首都是北京，去北京可以到天安门广场上看升国旗，可以通过百度的搜索更多详细信息。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3854755-456b-4eff-8fb6-a4bab1a50fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': 0.9879116,\n",
       "  'word': '中国的',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99999654,\n",
       "  'word': '北京',\n",
       "  'start': 6,\n",
       "  'end': 8},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9999956,\n",
       "  'word': '北京',\n",
       "  'start': 10,\n",
       "  'end': 12},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99982786,\n",
       "  'word': '天安门',\n",
       "  'start': 15,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9885372,\n",
       "  'word': '百度',\n",
       "  'start': 30,\n",
       "  'end': 32}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并实体后，天安门可以整体识别\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "classifier(\"中国的首都是北京，去北京可以到天安门广场上看升国旗，可以通过百度的搜索更多详细信息。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "797d9bb7-1875-47a4-8855-46585ada54d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9999173,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 0,\n",
       "  'end': 12},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.9999932,\n",
       "  'word': 'French',\n",
       "  'start': 18,\n",
       "  'end': 24},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99999696,\n",
       "  'word': 'New York City',\n",
       "  'start': 42,\n",
       "  'end': 55}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 英文合并实体对比\n",
    "classifier(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d8718-d1fc-4b1b-8eb6-999ef5e89617",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Question Answering(问答任务)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5db4e4c3-0365-4211-84f7-49cf7eb58128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.2947, start: 7, end: 15, answer: 每天早上6点左右\n"
     ]
    }
   ],
   "source": [
    "# 中文问答，提取式\n",
    "from transformers import AutoModelForQuestionAnswering,AutoTokenizer,pipeline\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('uer/roberta-base-chinese-extractive-qa')\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-chinese-extractive-qa')\n",
    "QA = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "QA_input = {'question': \"什么时候去天安门广场可以看到升国旗？\",'context': \"北京天安门广场每天早上6点左右会有升国旗仪式。\"}\n",
    "preds = QA(QA_input)\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08ed8070-74f4-487d-8fcf-8218fd18a610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.0, start: 3, end: 4, answer: 1\n"
     ]
    }
   ],
   "source": [
    "# 英文问答对比，没有回答出来\n",
    "QA_input = {'question': \"What is the capital of China?\",'context': \"On 1 October 1949, CCP Chairman Mao Zedong formally proclaimed the People's Republic of China in Tiananmen Square, Beijing.\"}\n",
    "preds = QA(QA_input)\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9210688-2cb1-4a4e-93eb-f8f1a4d0288c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.0312, start: 30, end: 54, answer: huggingface/transformers\n"
     ]
    }
   ],
   "source": [
    "# 英文问答对比，回答正确\n",
    "QA_input = {'question': \"What is the name of the repository?\",'context': \"The name of the repository is huggingface/transformers\"}\n",
    "preds = QA(QA_input)\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f4d9a-20e7-4725-a635-1a459b6aa211",
   "metadata": {},
   "source": [
    "### Summarization(文本摘要）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba6ffba0-c78a-4009-881c-91dc2cda38c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_59> 中国小米汽车su7在续航方面,搭载来自宁德时代的三元锂电池组。\n"
     ]
    }
   ],
   "source": [
    "# 中文摘要，试了几个效果不理想\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "\n",
    "article_text = \"\"\"小米汽车su7在续航方面，根据不同配置，分别搭载来自襄阳弗迪的磷酸铁锂电池组（比亚迪），以及来自宁德时代的三元锂电池组，电池容量分别为73.6千瓦时、94.3千瓦时和101千瓦时，其中73.6千瓦时对应的纯电续航里程700km，94.3千瓦时对应的纯电续航为830km，101千瓦时对应的纯电续航里程分别为800km（均为CLTC工况）。另外，小米SU7搭载了800V高压平台，配合宁德时代的麒麟电池，可以做到充电5分钟，220km续航，充电15分钟，510km续航的表现。\"\"\"\n",
    "\n",
    "model_name = \"csebuetnlp/mT5_m2m_crossSum_enhanced\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "get_lang_id = lambda lang: tokenizer._convert_token_to_id(\n",
    "    model.config.task_specific_params[\"langid_map\"][lang][1]\n",
    ") \n",
    "\n",
    "target_lang = \"chinese_simplified\" # for a list of available language names see below\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    [WHITESPACE_HANDLER(article_text)],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")[\"input_ids\"]\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    decoder_start_token_id=get_lang_id(target_lang),\n",
    "    max_length=84,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4,\n",
    ")[0]\n",
    "\n",
    "summary = tokenizer.decode(\n",
    "    output_ids,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d4aa5a3-a482-4b72-8937-a028318674d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_69> The first sequence transduction model based on attention has been revealed in the WMT 2014 English-to-French translation task.\n"
     ]
    }
   ],
   "source": [
    "# 英文摘要对比，总结关注重点不同\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "\n",
    "article_text = \"\"\"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, \n",
    "    replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. \n",
    "    For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. \n",
    "    On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. \n",
    "    In the former task our best model outperforms even all previously reported ensembles.\"\"\"\n",
    "\n",
    "model_name = \"csebuetnlp/mT5_m2m_crossSum_enhanced\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "get_lang_id = lambda lang: tokenizer._convert_token_to_id(\n",
    "    model.config.task_specific_params[\"langid_map\"][lang][1]\n",
    ") \n",
    "\n",
    "target_lang = \"english\" # for a list of available language names see below\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    [WHITESPACE_HANDLER(article_text)],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")[\"input_ids\"]\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    [WHITESPACE_HANDLER(article_text)],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")[\"input_ids\"]\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    decoder_start_token_id=get_lang_id(target_lang),\n",
    "    max_length=84,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4,\n",
    ")[0]\n",
    "\n",
    "summary = tokenizer.decode(\n",
    "    output_ids,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31143c4c-94b8-4a86-ba8b-fd402d111ddf",
   "metadata": {},
   "source": [
    "### Automatic speech recognition（自动语音识别）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e7ec6a-fa8f-435b-a407-bec7b3752c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "model_dir = \"./models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_decode = \"chinese\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f6e43-c880-4899-ac94-b70a8bc08618",
   "metadata": {},
   "source": [
    "#### openai/whisper-large-v2基础模型测试\n",
    "识别结果内容正确，但是为英文，没有输出中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a87e6d-dd8e-4c91-8b14-ee361f10cfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoTokenizer, AutoProcessor\n",
    "\n",
    "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "base_model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe45fc2-2fe3-4175-aa42-1a7d8fa68020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c735903-0b21-4c3f-a1ab-6ff44a188325",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = \"./data/audio/20240405_002217.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50382046-b870-402c-be91-1ab633179123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "pipeline_base = AutomaticSpeechRecognitionPipeline(model=base_model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language_decode, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeec9b54-5208-4aab-8c9d-ee1a5b2617e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Use complete data to compare the change of Twin Loss and Validation Loss in the training set. After the training is completed, use the test set to perform model assessment.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "with torch.cuda.amp.autocast():\n",
    "    text_base = pipeline_base(test_audio, max_new_tokens=255)[\"text\"]\n",
    "text_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24c751-7968-4659-a731-71ce59320fbf",
   "metadata": {},
   "source": [
    "#### openai/whisper-large-v2经过mozilla-foundation/common_voice_11_0中文语料进行lora微调后的测试\n",
    "经过中文语料进行lora微调后，可以输出中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8cc2322-ef03-4b83-aa2b-0cec879e3bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "\n",
    "peft_model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "383cb888-2e27-4739-a6df-3df941c4428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_base_peft = AutomaticSpeechRecognitionPipeline(model=peft_model, tokenizer=tokenizer, feature_extractor=feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b2c8ce-7a11-4a24-afaa-90093b58d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'使用完整的数据训练及对比Train Loss和Validation Loss变化，训练完成后使用测试级进行模型评估。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "with torch.cuda.amp.autocast():\n",
    "    text_base_peft = pipeline_base_peft(test_audio, max_new_tokens=255)[\"text\"]\n",
    "text_base_peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d75bcf-ce62-4ac2-a16b-0e4121d193e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
