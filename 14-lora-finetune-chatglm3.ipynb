{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false,
    "id": "89b89f64d8f8053d",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 微调配置\n",
    "- max_steps: 5000\n",
    "- per_device_train_batch_size: 1\n",
    "\n",
    "## 微调结果\n",
    "- 参见下面日志输出和截图\n",
    "\n",
    "## 相关代码\n",
    "- 微调：fintune_hf.py, config/lora.yaml\n",
    "- 推理：inference_hf.py\n",
    "- web gradio: web_demo_gradio.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false,
    "id": "a7bd9a514ed09ea6",
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 0. 环境检查\n",
    "首先，先检查代码的运行地址，确保运行地址处于 `finetune_demo` 中。\n",
    "并且，确保已经安装了 `requirements.txt`中的依赖。\n",
    "\n",
    "> 本 demo 中，不需要使用 deepspeed, mpi4py 两个依赖，如果您安装这两个依赖遇到问题，可以不安装这两个依赖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7703109d1443346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T05:29:22.200365Z",
     "start_time": "2024-04-14T05:29:22.080929Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/llmgit/ChatGLM3/finetune_demo\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50e92810011977",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T05:29:23.809255Z",
     "start_time": "2024-04-14T05:29:22.202731Z"
    },
    "cellView": "form",
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false,
    "id": "a1b7a99923349056",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:23:41.282431Z",
     "start_time": "2024-04-14T05:29:23.810692Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17c87410a24d844f",
    "outputId": "e347fc7d-875e-40c9-c682-3e064100476b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:35<00:00,  5.02s/it]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/root/miniconda3/envs/chatglm3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.5465, 'learning_rate': 4.99e-05, 'epoch': 0.0}                       \n",
      "{'loss': 5.0742, 'learning_rate': 4.9800000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.6754, 'learning_rate': 4.97e-05, 'epoch': 0.0}                       \n",
      "{'loss': 4.4848, 'learning_rate': 4.96e-05, 'epoch': 0.0}                       \n",
      "{'loss': 4.3545, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1383, 'learning_rate': 4.94e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.9314, 'learning_rate': 4.93e-05, 'epoch': 0.0}                       \n",
      "{'loss': 4.1219, 'learning_rate': 4.92e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.7979, 'learning_rate': 4.91e-05, 'epoch': 0.0}                       \n",
      "{'loss': 4.132, 'learning_rate': 4.9e-05, 'epoch': 0.0}                         \n",
      "{'loss': 4.2297, 'learning_rate': 4.89e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.8336, 'learning_rate': 4.88e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.7453, 'learning_rate': 4.87e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.5961, 'learning_rate': 4.86e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.6891, 'learning_rate': 4.85e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.5352, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.4295, 'learning_rate': 4.83e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.7527, 'learning_rate': 4.82e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.6691, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8336, 'learning_rate': 4.8e-05, 'epoch': 0.0}                        \n",
      "{'loss': 3.4422, 'learning_rate': 4.79e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.7471, 'learning_rate': 4.78e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.8217, 'learning_rate': 4.77e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.6621, 'learning_rate': 4.76e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.5625, 'learning_rate': 4.75e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.841, 'learning_rate': 4.74e-05, 'epoch': 0.0}                        \n",
      "{'loss': 3.6281, 'learning_rate': 4.73e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.7059, 'learning_rate': 4.72e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.7447, 'learning_rate': 4.71e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.5986, 'learning_rate': 4.7e-05, 'epoch': 0.0}                        \n",
      "{'loss': 3.516, 'learning_rate': 4.69e-05, 'epoch': 0.0}                        \n",
      "{'loss': 3.6945, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6187, 'learning_rate': 4.6700000000000003e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.4939, 'learning_rate': 4.660000000000001e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.4713, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5189, 'learning_rate': 4.64e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.2963, 'learning_rate': 4.630000000000001e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.4172, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.4637, 'learning_rate': 4.61e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.6346, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.3947, 'learning_rate': 4.5900000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.2082, 'learning_rate': 4.58e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.6137, 'learning_rate': 4.5700000000000006e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6064, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6326, 'learning_rate': 4.55e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.1547, 'learning_rate': 4.5400000000000006e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6215, 'learning_rate': 4.53e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.3633, 'learning_rate': 4.52e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.5553, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5865, 'learning_rate': 4.5e-05, 'epoch': 0.0}                        \n",
      " 10%|████                                    | 500/5000 [01:55<18:51,  3.98it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:47<00:47, 23.59s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [01:32<00:32, 32.82s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:38<00:00, 22.63s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.540 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 29.361262, 'eval_rouge-2': 5.66624, 'eval_rouge-l': 21.049901999999996, 'eval_bleu-4': 0.02607814182976195, 'eval_runtime': 147.1349, 'eval_samples_per_second': 0.34, 'eval_steps_per_second': 0.027, 'epoch': 0.0}\n",
      " 10%|████                                    | 500/5000 [04:22<18:51,  3.98it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:39<00:00, 22.63s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-500\n",
      "{'loss': 3.475, 'learning_rate': 4.49e-05, 'epoch': 0.0}                        \n",
      "{'loss': 3.5, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.0}            \n",
      "{'loss': 3.3937, 'learning_rate': 4.47e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.5293, 'learning_rate': 4.46e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.5568, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.635, 'learning_rate': 4.44e-05, 'epoch': 0.0}                        \n",
      "{'loss': 3.6738, 'learning_rate': 4.43e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.3371, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4324, 'learning_rate': 4.41e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.6354, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5449, 'learning_rate': 4.39e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.6338, 'learning_rate': 4.38e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.523, 'learning_rate': 4.3700000000000005e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1889, 'learning_rate': 4.36e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4809, 'learning_rate': 4.35e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4467, 'learning_rate': 4.3400000000000005e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.268, 'learning_rate': 4.33e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.3773, 'learning_rate': 4.32e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.5582, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.441, 'learning_rate': 4.3e-05, 'epoch': 0.01}                        \n",
      "{'loss': 3.5742, 'learning_rate': 4.29e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.2842, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4986, 'learning_rate': 4.27e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4193, 'learning_rate': 4.26e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.2627, 'learning_rate': 4.25e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4279, 'learning_rate': 4.24e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.2668, 'learning_rate': 4.23e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.5176, 'learning_rate': 4.22e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.6143, 'learning_rate': 4.21e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.5781, 'learning_rate': 4.2e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.4059, 'learning_rate': 4.19e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4928, 'learning_rate': 4.18e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3215, 'learning_rate': 4.17e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4408, 'learning_rate': 4.16e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3094, 'learning_rate': 4.15e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.443, 'learning_rate': 4.14e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.4508, 'learning_rate': 4.13e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3752, 'learning_rate': 4.12e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.1496, 'learning_rate': 4.11e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.6492, 'learning_rate': 4.1e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.302, 'learning_rate': 4.09e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.3271, 'learning_rate': 4.08e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3424, 'learning_rate': 4.07e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.0426, 'learning_rate': 4.0600000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5443, 'learning_rate': 4.05e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.424, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4518, 'learning_rate': 4.0300000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.7701, 'learning_rate': 4.02e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.1914, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3258, 'learning_rate': 4e-05, 'epoch': 0.01}                         \n",
      " 20%|███████▊                               | 1000/5000 [06:20<13:48,  4.83it/s]/root/miniconda3/envs/chatglm3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:09<00:09,  4.60s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:55<00:21, 21.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.481144, 'eval_rouge-2': 6.112642000000001, 'eval_rouge-l': 24.263340000000003, 'eval_bleu-4': 0.03052469007644036, 'eval_runtime': 106.0767, 'eval_samples_per_second': 0.471, 'eval_steps_per_second': 0.038, 'epoch': 0.01}\n",
      " 20%|███████▊                               | 1000/5000 [08:06<13:48,  4.83it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:58<00:00, 14.96s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-1000\n",
      "{'loss': 3.416, 'learning_rate': 3.99e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.4688, 'learning_rate': 3.9800000000000005e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2783, 'learning_rate': 3.97e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.5145, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3627, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.6025, 'learning_rate': 3.94e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.1881, 'learning_rate': 3.9300000000000007e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4953, 'learning_rate': 3.9200000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.7586, 'learning_rate': 3.91e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.2898, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.257, 'learning_rate': 3.8900000000000004e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.6316, 'learning_rate': 3.88e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3502, 'learning_rate': 3.8700000000000006e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.233, 'learning_rate': 3.86e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.3453, 'learning_rate': 3.85e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3354, 'learning_rate': 3.8400000000000005e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1764, 'learning_rate': 3.83e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4568, 'learning_rate': 3.82e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.1584, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4412, 'learning_rate': 3.8e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.3557, 'learning_rate': 3.79e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3975, 'learning_rate': 3.7800000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3721, 'learning_rate': 3.77e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.5717, 'learning_rate': 3.76e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.375, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3496, 'learning_rate': 3.74e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4607, 'learning_rate': 3.73e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.5459, 'learning_rate': 3.72e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.5236, 'learning_rate': 3.71e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.141, 'learning_rate': 3.7e-05, 'epoch': 0.01}                        \n",
      "{'loss': 3.7232, 'learning_rate': 3.69e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.2877, 'learning_rate': 3.68e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4297, 'learning_rate': 3.6700000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1031, 'learning_rate': 3.66e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4367, 'learning_rate': 3.65e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.2932, 'learning_rate': 3.6400000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5533, 'learning_rate': 3.63e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.5348, 'learning_rate': 3.62e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.1859, 'learning_rate': 3.61e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4998, 'learning_rate': 3.6e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.4273, 'learning_rate': 3.59e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3725, 'learning_rate': 3.58e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4613, 'learning_rate': 3.57e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4891, 'learning_rate': 3.56e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3838, 'learning_rate': 3.55e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.7, 'learning_rate': 3.54e-05, 'epoch': 0.01}                         \n",
      "{'loss': 3.1494, 'learning_rate': 3.53e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3619, 'learning_rate': 3.52e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4453, 'learning_rate': 3.51e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4113, 'learning_rate': 3.5e-05, 'epoch': 0.01}                       \n",
      " 30%|███████████▋                           | 1500/5000 [10:01<14:44,  3.96it/s]/root/miniconda3/envs/chatglm3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:47<00:47, 23.80s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [01:33<00:33, 33.08s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 28.581784000000003, 'eval_rouge-2': 5.9864440000000005, 'eval_rouge-l': 21.600834, 'eval_bleu-4': 0.029158928634602354, 'eval_runtime': 172.9376, 'eval_samples_per_second': 0.289, 'eval_steps_per_second': 0.023, 'epoch': 0.01}\n",
      " 30%|███████████▋                           | 1500/5000 [12:54<14:44,  3.96it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [02:05<00:00, 32.48s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-1500\n",
      "{'loss': 3.6568, 'learning_rate': 3.49e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4488, 'learning_rate': 3.48e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3037, 'learning_rate': 3.4699999999999996e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3039, 'learning_rate': 3.46e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.4164, 'learning_rate': 3.45e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.1373, 'learning_rate': 3.4399999999999996e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5008, 'learning_rate': 3.430000000000001e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2744, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.227, 'learning_rate': 3.41e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.2594, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3104, 'learning_rate': 3.3900000000000004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0189, 'learning_rate': 3.38e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.5047, 'learning_rate': 3.3700000000000006e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.176, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4432, 'learning_rate': 3.35e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.1887, 'learning_rate': 3.3400000000000005e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3584, 'learning_rate': 3.33e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.3754, 'learning_rate': 3.32e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.1725, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4268, 'learning_rate': 3.3e-05, 'epoch': 0.01}                       \n",
      "{'loss': 3.1463, 'learning_rate': 3.29e-05, 'epoch': 0.01}                      \n",
      "{'loss': 3.0773, 'learning_rate': 3.2800000000000004e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.5283, 'learning_rate': 3.27e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.5252, 'learning_rate': 3.26e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3816, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.4031, 'learning_rate': 3.24e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.8182, 'learning_rate': 3.2300000000000006e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.31, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.02}          \n",
      "{'loss': 3.3984, 'learning_rate': 3.21e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3941, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.3951, 'learning_rate': 3.19e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2322, 'learning_rate': 3.18e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2631, 'learning_rate': 3.1700000000000005e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2836, 'learning_rate': 3.16e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.1439, 'learning_rate': 3.15e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.5484, 'learning_rate': 3.1400000000000004e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2809, 'learning_rate': 3.13e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.4158, 'learning_rate': 3.12e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.1745, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.5039, 'learning_rate': 3.1e-05, 'epoch': 0.02}                       \n",
      "{'loss': 3.3018, 'learning_rate': 3.09e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3131, 'learning_rate': 3.08e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.4078, 'learning_rate': 3.07e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2791, 'learning_rate': 3.06e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2355, 'learning_rate': 3.05e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.1805, 'learning_rate': 3.04e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.6094, 'learning_rate': 3.03e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.0912, 'learning_rate': 3.02e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3576, 'learning_rate': 3.01e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.318, 'learning_rate': 3e-05, 'epoch': 0.02}                          \n",
      " 40%|███████████████▌                       | 2000/5000 [14:52<11:28,  4.36it/s]/root/miniconda3/envs/chatglm3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:08<00:08,  4.46s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:55<00:21, 21.84s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.91387799999999, 'eval_rouge-2': 7.027374, 'eval_rouge-l': 24.486554, 'eval_bleu-4': 0.0350997753827265, 'eval_runtime': 106.907, 'eval_samples_per_second': 0.468, 'eval_steps_per_second': 0.037, 'epoch': 0.02}\n",
      " 40%|███████████████▌                       | 2000/5000 [16:39<11:28,  4.36it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:59<00:00, 15.20s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-2000\n",
      "{'loss': 3.39, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.02}          \n",
      "{'loss': 3.3062, 'learning_rate': 2.98e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2053, 'learning_rate': 2.97e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3064, 'learning_rate': 2.96e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.7182, 'learning_rate': 2.95e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.1191, 'learning_rate': 2.94e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3395, 'learning_rate': 2.93e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.199, 'learning_rate': 2.9199999999999998e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.5904, 'learning_rate': 2.91e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3588, 'learning_rate': 2.9e-05, 'epoch': 0.02}                       \n",
      "{'loss': 3.4104, 'learning_rate': 2.8899999999999998e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.6061, 'learning_rate': 2.88e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.4893, 'learning_rate': 2.87e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.0375, 'learning_rate': 2.86e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3029, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.3801, 'learning_rate': 2.84e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2557, 'learning_rate': 2.83e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.4426, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2592, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.482, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.3211, 'learning_rate': 2.7900000000000004e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.0703, 'learning_rate': 2.7800000000000005e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.208, 'learning_rate': 2.7700000000000002e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.6025, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.3035, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.3854, 'learning_rate': 2.7400000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2885, 'learning_rate': 2.7300000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.4463, 'learning_rate': 2.7200000000000004e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2359, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.3062, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.3809, 'learning_rate': 2.6900000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.365, 'learning_rate': 2.6800000000000004e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.5199, 'learning_rate': 2.6700000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2773, 'learning_rate': 2.6600000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.1217, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.192, 'learning_rate': 2.64e-05, 'epoch': 0.02}                       \n",
      "{'loss': 3.2523, 'learning_rate': 2.6300000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.3652, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.4107, 'learning_rate': 2.61e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2037, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.3426, 'learning_rate': 2.5900000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.0961, 'learning_rate': 2.58e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.659, 'learning_rate': 2.57e-05, 'epoch': 0.02}                       \n",
      "{'loss': 3.6709, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2043, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2275, 'learning_rate': 2.54e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.5914, 'learning_rate': 2.5300000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.3508, 'learning_rate': 2.5200000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.3918, 'learning_rate': 2.51e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.4449, 'learning_rate': 2.5e-05, 'epoch': 0.02}                       \n",
      " 50%|███████████████████▌                   | 2500/5000 [18:38<09:16,  4.49it/s]/root/miniconda3/envs/chatglm3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:47<00:47, 23.91s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:54<00:16, 16.69s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 29.969147999999997, 'eval_rouge-2': 6.569882, 'eval_rouge-l': 22.446351999999997, 'eval_bleu-4': 0.030104972096046772, 'eval_runtime': 134.0421, 'eval_samples_per_second': 0.373, 'eval_steps_per_second': 0.03, 'epoch': 0.02}\n",
      " 50%|███████████████████▌                   | 2500/5000 [20:52<09:16,  4.49it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:26<00:00, 22.24s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-2500\n",
      "{'loss': 3.6654, 'learning_rate': 2.4900000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2387, 'learning_rate': 2.48e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.6057, 'learning_rate': 2.47e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2111, 'learning_rate': 2.46e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.4273, 'learning_rate': 2.45e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3943, 'learning_rate': 2.44e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.4826, 'learning_rate': 2.43e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3625, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.6201, 'learning_rate': 2.41e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3676, 'learning_rate': 2.4e-05, 'epoch': 0.02}                       \n",
      "{'loss': 3.3852, 'learning_rate': 2.39e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3271, 'learning_rate': 2.38e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3492, 'learning_rate': 2.37e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.5371, 'learning_rate': 2.36e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.0561, 'learning_rate': 2.35e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.1916, 'learning_rate': 2.3400000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.4309, 'learning_rate': 2.3300000000000004e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.4287, 'learning_rate': 2.32e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3937, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2672, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.0723, 'learning_rate': 2.29e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2607, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.5766, 'learning_rate': 2.2700000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.142, 'learning_rate': 2.26e-05, 'epoch': 0.02}                       \n",
      "{'loss': 3.3459, 'learning_rate': 2.25e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.1488, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2629, 'learning_rate': 2.23e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.0908, 'learning_rate': 2.22e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3715, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.5105, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2496, 'learning_rate': 2.19e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.4816, 'learning_rate': 2.18e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2602, 'learning_rate': 2.1700000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.9438, 'learning_rate': 2.16e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.3662, 'learning_rate': 2.15e-05, 'epoch': 0.02}                      \n",
      "{'loss': 3.2793, 'learning_rate': 2.1400000000000002e-05, 'epoch': 0.02}        \n",
      "{'loss': 3.2348, 'learning_rate': 2.13e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.5031, 'learning_rate': 2.12e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2992, 'learning_rate': 2.11e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2004, 'learning_rate': 2.1e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.2582, 'learning_rate': 2.09e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.5383, 'learning_rate': 2.08e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.1574, 'learning_rate': 2.07e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.3949, 'learning_rate': 2.06e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2848, 'learning_rate': 2.05e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2303, 'learning_rate': 2.04e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.3166, 'learning_rate': 2.0300000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.2566, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.1793, 'learning_rate': 2.01e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.3023, 'learning_rate': 2e-05, 'epoch': 0.03}                         \n",
      " 60%|███████████████████████▍               | 3000/5000 [22:54<07:50,  4.25it/s]/root/miniconda3/envs/chatglm3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:47<00:47, 23.93s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:53<00:16, 16.49s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.809946000000004, 'eval_rouge-2': 6.9396640000000005, 'eval_rouge-l': 23.258858, 'eval_bleu-4': 0.034856894581312164, 'eval_runtime': 106.6075, 'eval_samples_per_second': 0.469, 'eval_steps_per_second': 0.038, 'epoch': 0.03}\n",
      " 60%|███████████████████████▍               | 3000/5000 [24:41<07:50,  4.25it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:58<00:00, 12.08s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-3000\n",
      "{'loss': 3.4924, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.0684, 'learning_rate': 1.9800000000000004e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.1871, 'learning_rate': 1.97e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.3326, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.2027, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.4066, 'learning_rate': 1.94e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.5811, 'learning_rate': 1.93e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.5012, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.5002, 'learning_rate': 1.91e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2373, 'learning_rate': 1.9e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.2525, 'learning_rate': 1.8900000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.1773, 'learning_rate': 1.88e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.0893, 'learning_rate': 1.87e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.3109, 'learning_rate': 1.86e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.5158, 'learning_rate': 1.85e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.4504, 'learning_rate': 1.84e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.559, 'learning_rate': 1.83e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.2926, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.4611, 'learning_rate': 1.81e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.1221, 'learning_rate': 1.8e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.323, 'learning_rate': 1.79e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.4584, 'learning_rate': 1.78e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.1936, 'learning_rate': 1.77e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2369, 'learning_rate': 1.76e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.4061, 'learning_rate': 1.75e-05, 'epoch': 0.03}                      \n",
      " 65%|█████████████████████████▍             | 3256/5000 [25:43<06:57,  4.17it/s]\n",
      " 50%|██████████████████████▌                      | 2/4 [00:47<00:47, 23.94s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:54<00:16, 16.93s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.376414, 'eval_rouge-2': 6.646678000000001, 'eval_rouge-l': 23.862362000000005, 'eval_bleu-4': 0.03286806398527939, 'eval_runtime': 109.4392, 'eval_samples_per_second': 0.457, 'eval_steps_per_second': 0.037, 'epoch': 0.03}\n",
      " 70%|███████████████████████████▎           | 3500/5000 [28:31<06:39,  3.76it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:01<00:00, 12.98s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-3500\n",
      "{'loss': 3.4188, 'learning_rate': 1.49e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2564, 'learning_rate': 1.48e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.3127, 'learning_rate': 1.47e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2057, 'learning_rate': 1.4599999999999999e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.275, 'learning_rate': 1.45e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.1523, 'learning_rate': 1.44e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.4527, 'learning_rate': 1.43e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.0904, 'learning_rate': 1.42e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.065, 'learning_rate': 1.4099999999999999e-05, 'epoch': 0.03}         \n",
      "{'loss': 3.3213, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.4371, 'learning_rate': 1.3900000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.3627, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.4523, 'learning_rate': 1.3700000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.3482, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.4553, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.2086, 'learning_rate': 1.3400000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.2516, 'learning_rate': 1.3300000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.3154, 'learning_rate': 1.32e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2309, 'learning_rate': 1.3100000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.3795, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.3061, 'learning_rate': 1.29e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2107, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.3234, 'learning_rate': 1.27e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2398, 'learning_rate': 1.2600000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.0363, 'learning_rate': 1.25e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2252, 'learning_rate': 1.24e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.4111, 'learning_rate': 1.23e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2324, 'learning_rate': 1.22e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.1715, 'learning_rate': 1.2100000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.4256, 'learning_rate': 1.2e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.4795, 'learning_rate': 1.19e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.4629, 'learning_rate': 1.18e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.0826, 'learning_rate': 1.1700000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.3504, 'learning_rate': 1.16e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2977, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 2.9667, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.4588, 'learning_rate': 1.13e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2096, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.1885, 'learning_rate': 1.11e-05, 'epoch': 0.03}                      \n",
      "{'loss': 2.9475, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.3275, 'learning_rate': 1.09e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.091, 'learning_rate': 1.08e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.3662, 'learning_rate': 1.0700000000000001e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.0947, 'learning_rate': 1.06e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.1855, 'learning_rate': 1.05e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2363, 'learning_rate': 1.04e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.2971, 'learning_rate': 1.03e-05, 'epoch': 0.03}                      \n",
      "{'loss': 3.317, 'learning_rate': 1.02e-05, 'epoch': 0.03}                       \n",
      "{'loss': 3.1236, 'learning_rate': 1.0100000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.641, 'learning_rate': 1e-05, 'epoch': 0.03}                          \n",
      " 80%|███████████████████████████████▏       | 4000/5000 [30:32<03:36,  4.61it/s]/root/miniconda3/envs/chatglm3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:47<00:47, 23.95s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [01:34<00:33, 33.29s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.500254000000005, 'eval_rouge-2': 6.9204, 'eval_rouge-l': 24.76609, 'eval_bleu-4': 0.033747875986219894, 'eval_runtime': 105.9545, 'eval_samples_per_second': 0.472, 'eval_steps_per_second': 0.038, 'epoch': 0.03}\n",
      " 80%|███████████████████████████████▏       | 4000/5000 [32:18<03:36,  4.61it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:38<00:00, 22.24s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-4000\n",
      "{'loss': 3.3836, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.03}         \n",
      "{'loss': 3.0773, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.1127, 'learning_rate': 9.7e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2428, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.3639, 'learning_rate': 9.5e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.4033, 'learning_rate': 9.4e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.1145, 'learning_rate': 9.3e-06, 'epoch': 0.04}                       \n",
      "{'loss': 2.9955, 'learning_rate': 9.2e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2973, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.3141, 'learning_rate': 9e-06, 'epoch': 0.04}                         \n",
      "{'loss': 3.2432, 'learning_rate': 8.9e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.1814, 'learning_rate': 8.8e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.0555, 'learning_rate': 8.7e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.5656, 'learning_rate': 8.599999999999999e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.601, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.04}          \n",
      "{'loss': 3.3232, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.2262, 'learning_rate': 8.3e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2529, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.4396, 'learning_rate': 8.1e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.4525, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.1564, 'learning_rate': 7.9e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.1572, 'learning_rate': 7.8e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.4029, 'learning_rate': 7.7e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3707, 'learning_rate': 7.6e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3732, 'learning_rate': 7.5e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2877, 'learning_rate': 7.4e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3527, 'learning_rate': 7.2999999999999996e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.1619, 'learning_rate': 7.2e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3641, 'learning_rate': 7.1e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.4008, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.2041, 'learning_rate': 6.900000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.2408, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.2569, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.2643, 'learning_rate': 6.6e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2166, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.225, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.0596, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.4287, 'learning_rate': 6.2e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.4477, 'learning_rate': 6.1e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.5377, 'learning_rate': 6e-06, 'epoch': 0.04}                         \n",
      "{'loss': 3.1646, 'learning_rate': 5.9e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3459, 'learning_rate': 5.8e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2336, 'learning_rate': 5.7000000000000005e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.3543, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.2687, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.476, 'learning_rate': 5.4e-06, 'epoch': 0.04}                        \n",
      "{'loss': 3.3455, 'learning_rate': 5.3e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2541, 'learning_rate': 5.2e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3555, 'learning_rate': 5.1e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.1775, 'learning_rate': 5e-06, 'epoch': 0.04}                         \n",
      " 90%|███████████████████████████████████    | 4500/5000 [34:18<01:47,  4.63it/s]/root/miniconda3/envs/chatglm3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:08<00:08,  4.02s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:54<00:21, 21.68s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.713175999999997, 'eval_rouge-2': 6.88911, 'eval_rouge-l': 24.90342, 'eval_bleu-4': 0.03385224633532818, 'eval_runtime': 107.6103, 'eval_samples_per_second': 0.465, 'eval_steps_per_second': 0.037, 'epoch': 0.04}\n",
      " 90%|███████████████████████████████████    | 4500/5000 [36:06<01:47,  4.63it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:59<00:00, 15.50s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-4500\n",
      "{'loss': 3.2555, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.2906, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.3473, 'learning_rate': 4.7e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.349, 'learning_rate': 4.6e-06, 'epoch': 0.04}                        \n",
      "{'loss': 3.2412, 'learning_rate': 4.5e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.216, 'learning_rate': 4.4e-06, 'epoch': 0.04}                        \n",
      "{'loss': 3.2309, 'learning_rate': 4.2999999999999995e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.0291, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.5365, 'learning_rate': 4.1000000000000006e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.3598, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.1031, 'learning_rate': 3.9e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2707, 'learning_rate': 3.8e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2441, 'learning_rate': 3.7e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.1953, 'learning_rate': 3.6e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3006, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.1658, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.2305, 'learning_rate': 3.3e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3209, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.104, 'learning_rate': 3.1e-06, 'epoch': 0.04}                        \n",
      "{'loss': 3.2354, 'learning_rate': 3e-06, 'epoch': 0.04}                         \n",
      "{'loss': 3.3764, 'learning_rate': 2.9e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.1301, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.0719, 'learning_rate': 2.7e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2799, 'learning_rate': 2.6e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3172, 'learning_rate': 2.5e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.0027, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.26, 'learning_rate': 2.3e-06, 'epoch': 0.04}                         \n",
      "{'loss': 3.365, 'learning_rate': 2.2e-06, 'epoch': 0.04}                        \n",
      "{'loss': 3.2467, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.3984, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.1527, 'learning_rate': 1.9e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3449, 'learning_rate': 1.8e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.173, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.758, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.04}         \n",
      "{'loss': 3.4455, 'learning_rate': 1.5e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2305, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.2493, 'learning_rate': 1.3e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.2094, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.4186, 'learning_rate': 1.1e-06, 'epoch': 0.04}                       \n",
      "{'loss': 3.3367, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}        \n",
      "{'loss': 3.257, 'learning_rate': 9e-07, 'epoch': 0.04}                          \n",
      "{'loss': 3.2004, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.04}         \n",
      "{'loss': 3.2604, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.04}         \n",
      "{'loss': 3.266, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.04}          \n",
      "{'loss': 3.3979, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.04}         \n",
      "{'loss': 3.2215, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.04}        \n",
      "{'loss': 3.3238, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.04}        \n",
      "{'loss': 3.3191, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.04}        \n",
      "{'loss': 3.4174, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.04}        \n",
      "{'loss': 3.1777, 'learning_rate': 0.0, 'epoch': 0.04}                           \n",
      "100%|███████████████████████████████████████| 5000/5000 [38:08<00:00,  3.87it/s]/root/miniconda3/envs/chatglm3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:47<00:47, 23.97s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:55<00:17, 17.08s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.23729, 'eval_rouge-2': 7.188968, 'eval_rouge-l': 25.321624000000007, 'eval_bleu-4': 0.03544813735459273, 'eval_runtime': 108.9752, 'eval_samples_per_second': 0.459, 'eval_steps_per_second': 0.037, 'epoch': 0.04}\n",
      "100%|███████████████████████████████████████| 5000/5000 [39:57<00:00,  3.87it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:01<00:00, 12.74s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-5000\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2398.2214, 'train_samples_per_second': 2.085, 'train_steps_per_second': 2.085, 'train_loss': 3.3761169921875, 'epoch': 0.04}\n",
      "100%|███████████████████████████████████████| 5000/5000 [39:58<00:00,  2.08it/s]\n",
      "/root/miniconda3/envs/chatglm3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [27:13<00:00, 24.38s/it]\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python finetune_hf.py  data/AdvertiseGen_fix  models/chatglm3-6b  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "collapsed": false,
    "id": "d9418f6c5c264601",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:23:52.725227Z",
     "start_time": "2024-04-14T06:23:41.284552Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5060015c24e97ae",
    "outputId": "d3f03d0d-46bf-4c74-9b00-dc0160da0e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:04<00:00,  1.72it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "时尚又百变的网纱连衣裙，不规则的拼接设计，展现出女性优雅又迷人的气质。木耳边的设计，性感又时尚，视觉上拉长身形，凸显出身材曲线。拉链套头设计，方便穿脱，又美观大方。下摆的百褶设计，更显瘦，气质又优雅。\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python inference_hf.py output/checkpoint-5000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9985707-f881-4dcc-b29d-5fc949f341fb",
   "metadata": {},
   "source": [
    "使用原始模型进行推理对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e4cbff-f7d8-4153-b34c-8000a76acc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:04<00:00,  1.72it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "这是一款性感的网纱百褶连衣裙，裙下摆采用了压褶设计，使得裙摆更加飘逸。衣服采用套头设计，方便穿脱。裙衣门襟采用了拉链和木耳边设计，增加了裙子的时尚感。整体款式为不规则设计，更具个性化。这款连衣裙的显瘦效果非常明显，能够很好地展现女性的优美曲线。\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python inference_hf.py models/chatglm3-6b/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4ecf4-ee4e-43ca-9dc5-398f3ad478a3",
   "metadata": {},
   "source": [
    "针对web_demo_gradio.py使用微调后的模型进行推理\n",
    "![](webdemogradio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90615374-2091-4107-8340-fd83c0959506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:04<00:00,  1.63it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Running on local URL:  http://127.0.0.1:7870\n",
      "Running on public URL: https://b0e5dc2c2d43a9a749.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "\n",
      "\n",
      "====conversation====\n",
      " [{'role': 'user', 'content': '类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则'}]\n",
      "\n",
      "\n",
      "====conversation====\n",
      " [{'role': 'user', 'content': '类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则'}, {'role': 'assistant', 'content': '不规则的<UNK>，让整件连衣裙更具有时尚感。加上仿若小碎花图案的面料，让整个裙子看起来更加精致细腻，整体的设计采用圆领和袖口以及胸前点缀着透视网纱元素，使得整件衣服显得十分性感迷人。同时腰间还采用了松紧带设计，可以随意调节腰部的大小，穿着起来特别的舒适方便。'}, {'role': 'user', 'content': '类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则'}]\n",
      "\n",
      "\n",
      "====conversation====\n",
      " [{'role': 'user', 'content': '类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则'}, {'role': 'assistant', 'content': '不规则的<UNK>，让整件连衣裙更具有时尚感。加上仿若小碎花图案的面料，让整个裙子看起来更加精致细腻，整体的设计采用圆领和袖口以及胸前点缀着透视网纱元素，使得整件衣服显得十分性感迷人。同时腰间还采用了松紧带设计，可以随意调节腰部的大小，穿着起来特别的舒适方便。'}, {'role': 'user', 'content': '类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则'}, {'role': 'assistant', 'content': '一款很显气质的小碎花印花连衣裙，上身效果非常美丽大方，很有女人味。而肩部的不规则荷叶边的拼接，又增添了一丝俏皮可爱气息。并且这款连衣裙是套头的，穿脱简单方便，而且还可以凸显出你修长的颈部线条哦~'}, {'role': 'user', 'content': '类型#裙版型#显瘦材质#网纱风格#性感裙型#百褶裙下摆#压褶裙长#连衣裙裙衣门襟#拉链裙衣门襟#套头裙款式#拼接裙款式#拉链裙款式#木耳边裙款式#抽褶*裙款式#不规则'}]\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7870 <> https://b0e5dc2c2d43a9a749.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python web_demo_gradio.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false,
    "id": "18cd83087f096094",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
